{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc853d46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/05 20:14:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "21/12/05 20:14:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/12/05 20:14:47 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "21/12/05 20:14:47 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:919)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:154)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:262)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:169)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import ArrayType\n",
    "import re\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "spark = SparkSession.builder.master(\"spark://sparkmaster:7077\").appName(\"extract_wiki_dict\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f274cd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract text tags from xml\n",
    "initial_df = spark.read.format('xml').options(rowTag='page').load('./wiki.xml')\n",
    "df = initial_df.selectExpr(\"revision.text._VALUE as text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0cea0f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[regexp_replace(text, (<ref.+?/>)|(<ref.+?</ref>)|(<!--.+?-->)|(\\s?(\\(([^()])*\\))), , 1): string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clean text\n",
    "import pyspark.sql.functions as sqlf\n",
    "df.select(sqlf.regexp_replace('text', r'(<ref.+?/>)|(<ref.+?</ref>)|(<!--.+?-->)|(\\s?(\\(([^()])*\\)))', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af456dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize to sentences\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "sen_tok = RegexTokenizer(inputCol='text', outputCol='sentences', gaps=False, pattern=r'((\\s|^)\\'*[A-Z].+?[.!?])(?=\\s+\\S*[A-Z]|$)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16bf8f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    results = re.findall(r'[^\\s!,.?\":;]+', sentence)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf33261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6519cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentences(sentence_tuples):\n",
    "    entries = []\n",
    "    for sentence_tuple in sentence_tuples:\n",
    "        original = sentence_tuple[0]\n",
    "        # print(f'Origin: {original}')\n",
    "        processed = re.sub(r'(\\'\\'\\'?)|(^\\s)|(<.+?>)|(</.+?>)', '', original)\n",
    "        results = extract_links_to_dict(processed)\n",
    "        processed = re.sub(r'(\\[\\[[^]]*\\|)|(]])|(\\[\\[)', '', processed)\n",
    "        print(f'Clear: {processed}')\n",
    "        lemmatized = lemmatize_sentence(processed, results)\n",
    "        print(f'Lemmatized: {lemmatized}')\n",
    "        entries.append({\n",
    "            \"original\": original,\n",
    "            \"processed\": processed,\n",
    "            \"lemmatized\": lemmatized\n",
    "        })\n",
    "\n",
    "    return entries\n",
    "\n",
    "def parse_text_to_sentences(text):\n",
    "    text = re.sub(r'(<ref.+?/>)|(<ref.+?</ref>)|(<!--.+?-->)|(\\s?(\\(([^()])*\\)))', '', text)\n",
    "    sentences = re.findall(r'((\\s|^)\\'*[A-Z].+?[.!?])(?=\\s+\\S*[A-Z]|$)', text)\n",
    "    # sentences = re.findall(r'(\\S*[A-Z].+?[.!?])(?=\\s+\\S*[A-Z]|$)', text)\n",
    "    # sentences = re.findall(r'(\\S*[A-Z].+?(\\(.+?\\))?[.!?])(?=\\s+\\S*[A-Z]|$)', text)\n",
    "    # sentences = re.findall(r'(?![a-z])*', text)\n",
    "    entries = clean_sentences(sentences)\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2faff0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131.79812622070312\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def parse_page(page_string):\n",
    "    text = parse_text_from_xml(page_string)\n",
    "    entries = parse_text_to_sentences(text)\n",
    "    return entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d52fe16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_text_from_xml(xml_string):\n",
    "    root = ElementTree.fromstring(xml_string)\n",
    "    text_element = root.find('revision/text')\n",
    "    return text_element.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57437cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(sentence, link_dict):\n",
    "\n",
    "    words = tokenize_sentence(sentence)\n",
    "    processed = ''\n",
    "    for word in words:\n",
    "        lemma = simplemma.lemmatize(word, lang_data)\n",
    "        processed += lemma + ' '\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05228949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dict_pair_valid(dict_pair):\n",
    "    # If the starting letter differs, the pair is not valid\n",
    "    if not dict_pair['base'][0] == dict_pair['form'][0]:\n",
    "        return False\n",
    "    # If the number of words differ, the pair is not valid\n",
    "    if not len(dict_pair['base'].split(' ')) == len(dict_pair['form'].split(' ')):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def extract_links_to_dict(sentence):\n",
    "    dict_results = []\n",
    "    results = re.findall(r'\\[\\[[A-Za-z0-9.]+?\\|.+?]]', sentence)\n",
    "    if results:\n",
    "        for result in results:\n",
    "            dict_result = {\n",
    "                'base': re.findall(r'\\[\\[(.+?)\\|', result)[0],\n",
    "                'form': re.findall(r'\\|(.+?)]]', result)[0],\n",
    "                'postfix': ''\n",
    "            }\n",
    "            if is_dict_pair_valid(dict_result):\n",
    "                dict_results.append(dict_result)\n",
    "    if not results:\n",
    "        results = re.findall(r'\\[\\[[A-Za-z0-9.]+?]][a-z]+?\\s', sentence)\n",
    "        if results:\n",
    "            for result in results:\n",
    "                dict_result = {\n",
    "                    'base': re.findall(r'\\[\\[(.+?)]]', result)[0],\n",
    "                    'postfix': re.findall(r'\\[\\[.+?]](.*)\\s', result)[0]\n",
    "                }\n",
    "                dict_result['form'] = dict_result['base'] + dict_result['postfix']\n",
    "                dict_results.append(dict_result)\n",
    "\n",
    "    if results:\n",
    "        print(dict_results)\n",
    "    save_to_terms_dictionary(dict_results)\n",
    "    return dict_results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
